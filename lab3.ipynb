{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3d6a79-c1a9-4f34-bb89-ed2bdf5eb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "from torchvision import datasets, models \n",
    "from torch.utils.data import DataLoader \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0bf7ba-481b-426f-b198-84ebfd624f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Downloads/Image-Classification-main/Image-Classification-main/dataset\" \n",
    "test_dir = \"Downloads/Image-Classification-main/Image-Classification-main/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e901a0e-2b57-4ae5-b58d-0bab5a07544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAHE\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\MAHE\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\MAHE/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:00<00:00, 65.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform) \n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) \n",
    "class_names = train_dataset.classes \n",
    "model = models.resnet18(pretrained=True) \n",
    "num_features = model.fc.in_features \n",
    "model.fc = nn.Linear(num_features, len(class_names)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8ab9492-11de-4f9d-aa4b-51e5a0c21e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f936919-3c21-4f8d-ae7c-69110954ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0522\n",
      "Epoch 2, Loss: 0.0571\n",
      "Epoch 3, Loss: 0.0559\n",
      "Epoch 4, Loss: 0.0486\n",
      "Epoch 5, Loss: 0.0474\n",
      "Epoch 6, Loss: 0.0497\n",
      "Epoch 7, Loss: 0.0441\n",
      "Epoch 8, Loss: 0.0529\n",
      "Epoch 9, Loss: 0.0491\n",
      "Epoch 10, Loss: 0.0486\n",
      "Epoch 11, Loss: 0.0494\n",
      "Epoch 12, Loss: 0.0476\n",
      "Epoch 13, Loss: 0.0502\n",
      "Epoch 14, Loss: 0.0500\n",
      "Epoch 15, Loss: 0.0469\n",
      "Epoch 16, Loss: 0.0501\n",
      "Epoch 17, Loss: 0.0443\n",
      "Epoch 18, Loss: 0.0516\n",
      "Epoch 19, Loss: 0.0480\n",
      "Epoch 20, Loss: 0.0441\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20): \n",
    "    model.train() \n",
    "    running_loss = 0.0 \n",
    "    for inputs, labels in train_loader: \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(inputs) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        running_loss += loss.item() \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f4cb9c-df00-4f2b-82c7-30816ad9cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.jpeg → sad\n",
      "b.jpeg → sad\n",
      "c.jpeg → happy\n",
      "d.jpeg → sad\n",
      "e.jpeg → sad\n",
      "f.jpeg → happy\n",
      "g.jpeg → sad\n",
      "h.jpeg → happy\n",
      "i.jpeg → sad\n",
      "j.jpeg → happy\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "model.eval() \n",
    "with torch.no_grad(): \n",
    "    for img_name in os.listdir(test_dir): \n",
    "        img_path = os.path.join(test_dir, img_name) \n",
    "        if not img_name.lower().endswith((\".jpg\", \".png\", \".jpeg\")): \n",
    "            continue\n",
    "        img = Image.open(img_path).convert(\"RGB\") \n",
    "        img = transform(img).unsqueeze(0)\n",
    "\n",
    "        outputs = model(img) \n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        label = class_names[predicted.item()] \n",
    "        print(f\"{img_name} → {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d087fc-e2e4-4b9b-95a5-5667a5d40313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
